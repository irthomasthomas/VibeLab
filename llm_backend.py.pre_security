#!/usr/bin/env python3
"""
VibeLab Enhanced Backend - Unified LLM + Database API
Combines LLM generation with persistent database storage
"""

import json
import subprocess
import sys
import time
import uuid
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import parse_qs, urlparse
from database_manager import DatabaseManager
import logging
import asyncio
from typing import Optional, Tuple, Dict, Any
from concurrent.futures import ThreadPoolExecutor
import threading

# Common prompt types for validation
COMMON_PROMPT_TYPES = [
    'base', 'baseline', 'modified', 'system', 'multi_step',
    'role_play', 'chain_of_thought', 'few_shot', 'custom'
]

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ImprovedLLMExecutor:
    """Enhanced LLM executor with better subprocess handling"""
    
    def __init__(self, max_workers: int = 5, default_timeout: int = 120):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.default_timeout = default_timeout
        self.active_processes = {}
        self.lock = threading.Lock()
    
    def execute_llm_sync(self, 
                        model: str, 
                        prompt: str, 
                        conversation_id: Optional[str] = None,
                        timeout: Optional[int] = None) -> Tuple[str, int, Optional[str]]:
        """
        Execute LLM command synchronously with improved error handling
        
        Returns: (output, generation_time_ms, new_conversation_id)
        """
        timeout = timeout or self.default_timeout
        cmd = ['llm', '-m', model]
        
        if conversation_id:
            cmd.extend(['-c', conversation_id])
        
        logger.info(f"Executing LLM: model='{model}', prompt_len={len(prompt)}, conv_id={conversation_id}")
        
        start_time = time.time()
        process = None
        
        try:
            # Start process
            process = subprocess.Popen(
                cmd,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,  # Line buffered
                universal_newlines=True
            )
            
            # Track active process
            with self.lock:
                self.active_processes[process.pid] = process
            
            # Communicate with timeout
            stdout, stderr = process.communicate(input=prompt, timeout=timeout)
            
            generation_time_ms = int((time.time() - start_time) * 1000)
            
            if process.returncode != 0:
                error_msg = f"LLM command failed (code {process.returncode}): {stderr}"
                logger.error(error_msg)
                raise RuntimeError(error_msg)
            
            # Parse response
            output = stdout.strip()
            
            # Try to extract conversation ID from stderr or special markers
            new_conversation_id = self._extract_conversation_id(stderr)
            if not new_conversation_id and not conversation_id:
                # Generate a placeholder ID if none found
                new_conversation_id = f"conv_{int(time.time())}_{model.replace('/', '_')}"
            elif not new_conversation_id:
                new_conversation_id = conversation_id
            
            logger.info(f"LLM success: output_len={len(output)}, time={generation_time_ms}ms, conv_id={new_conversation_id}")
            return output, generation_time_ms, new_conversation_id
            
        except subprocess.TimeoutExpired:
            logger.error(f"LLM timeout after {timeout}s")
            if process:
                process.terminate()
                # Give it 5 seconds to terminate gracefully
                try:
                    process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    process.kill()
            raise TimeoutError(f"LLM command timed out after {timeout} seconds")
            
        except Exception as e:
            logger.error(f"LLM execution error: {e}")
            raise
            
        finally:
            # Clean up tracking
            if process:
                with self.lock:
                    self.active_processes.pop(process.pid, None)
    
    def _extract_conversation_id(self, stderr: str) -> Optional[str]:
        """
        Try to extract conversation ID from stderr output
        Different LLM providers might output this differently
        """
        if not stderr:
            return None
        
        # Look for patterns like "Conversation: <id>" or "conversation_id: <id>"
        import re
        patterns = [
            r'[Cc]onversation[:\s]+([a-zA-Z0-9_-]+)',
            r'conversation_id[:\s]+([a-zA-Z0-9_-]+)',
            r'conv_id[:\s]+([a-zA-Z0-9_-]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, stderr)
            if match:
                return match.group(1)
        
        return None
    
    def cancel_all(self):
        """Cancel all active LLM processes"""
        with self.lock:
            for pid, process in list(self.active_processes.items()):
                try:
                    process.terminate()
                    logger.info(f"Terminated LLM process {pid}")
                except:
                    pass
            self.active_processes.clear()
    
    def __del__(self):
        """Cleanup on deletion"""
        self.cancel_all()
        self.executor.shutdown(wait=True)


class EnhancedVibeLab(BaseHTTPRequestHandler):
    
    # Class-level LLM executor shared across all requests
    llm_executor = ImprovedLLMExecutor(max_workers=5, default_timeout=120)
    
    def __init__(self, *args, **kwargs):
        self.db = DatabaseManager()
        super().__init__(*args, **kwargs)
    
    def do_OPTIONS(self):
        """Handle CORS preflight"""
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'POST, GET, PUT, DELETE, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()
    
    def send_json_response(self, data, status=200):
        """Send JSON response with CORS headers"""
        self.send_response(status)
        self.send_header('Content-Type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        self.wfile.write(json.dumps(data).encode('utf-8'))
    
    def validate_prompt_type(self, prompt_type):
        """Validate and normalize prompt_type"""
        if not prompt_type:
            return 'base'
        
        # Clean the type - remove special chars, convert to lowercase
        clean_type = prompt_type.lower().strip().replace('-', '_').replace(' ', '_')
        
        # Log for debugging
        logger.info(f"Validating prompt_type: '{prompt_type}' -> '{clean_type}'")
        
        return clean_type

    def send_error_response(self, error_msg, status=500):
        """Send error response"""
        # Log the full error on the server side as well
        logger.error(f"Sending error response to client: {status} - {error_msg}")
        self.send_json_response({
            'success': False,
            'error': error_msg
        }, status)
    
    def parse_request_body(self):
        """Parse JSON request body"""
        try:
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length == 0:
                return {}
            
            post_data = self.rfile.read(content_length)
            return json.loads(post_data.decode('utf-8'))
        except Exception as e:
            logger.error(f"Error parsing request body: {e}")
            raise ValueError(f"Invalid JSON in request body: {e}")
    
    def execute_llm(self, model, prompt, conversation_id=None):
        """Execute llm command using the improved executor"""
        try:
            output, gen_time, new_conv_id = self.llm_executor.execute_llm_sync(
                model, prompt, conversation_id
            )
            
            # For backward compatibility, return just output and time
            # The new conversation ID could be stored or returned separately if needed
            return output, gen_time
            
        except TimeoutError as e:
            logger.error(f"LLM timeout: {e}")
            raise Exception(str(e))
        except Exception as e:
            logger.error(f"LLM error: {e}")
            raise Exception(f"LLM execution failed: {str(e)}")
    
    def do_POST(self):
        """Handle POST requests"""
        path = urlparse(self.path).path
        logger.info(f"POST request received for: {path}")
        
        try:
            data = self.parse_request_body()
            logger.info(f"POST data for {path}: {json.dumps(data)[:200]}...")
            
            if path == '/generate':
                # Basic generation endpoint
                prompt = data.get('prompt', '')
                model = data.get('model', 'gpt-4')
                conversation_id = data.get('conversation_id')
                experiment_id = data.get('experiment_id')
                
                if not prompt:
                    self.send_error_response('No prompt provided', 400)
                    return
                
                output, generation_time = self.execute_llm(model, prompt, conversation_id)
                response = {
                    'success': True,
                    'output': output,
                    'generation_time_ms': generation_time
                }
                
                # Save to database if experiment_id provided
                if experiment_id:
                    logger.info(f"Saving generation for experiment_id: {experiment_id}")
                    existing_model = self.db.get_model_by_name(model)
                    if not existing_model:
                        model_id = self.db.register_model(model)
                        logger.info(f"Registered new model '{model}' with id: {model_id}")
                    else:
                        model_id = existing_model['id']
                    
                    prompt_id_from_payload = data.get('prompt_id')
                    if not prompt_id_from_payload:
                        current_prompt_type = self.validate_prompt_type(data.get('prompt_type', 'base'))
                        logger.info(f"No prompt_id in payload. Creating new prompt. Type: '{current_prompt_type}', Content: '{prompt[:50]}...'")
                        
                        # **** Critical section for CHECK constraint ****
                        try:
                            prompt_id_created = self.db.create_prompt(
                                experiment_id=experiment_id,
                                content=prompt,
                                prompt_type=current_prompt_type 
                            )
                            logger.info(f"Successfully created prompt with id: {prompt_id_created}, type: {current_prompt_type}")
                        except Exception as e_prompt:
                            logger.exception(f"ERROR creating prompt in DB! Experiment_id: {experiment_id}, Type: '{current_prompt_type}', Content: '{prompt[:50]}...'")
                            # Re-raise to be caught by outer try-except and send error response
                            raise e_prompt 
                        prompt_id_to_use = prompt_id_created
                    else:
                        prompt_id_to_use = prompt_id_from_payload
                        logger.info(f"Using existing prompt_id from payload: {prompt_id_to_use}")

                    generation_id = self.db.save_generation(
                        experiment_id=experiment_id,
                        prompt_id=prompt_id_to_use,
                        model_id=model_id,
                        output=output,
                        generation_time_ms=generation_time,
                        conversation_id=conversation_id,
                        metadata=data.get('metadata', {})
                    )
                    response['generation_id'] = generation_id
                    logger.info(f"Saved generation with id: {generation_id}")
                
                self.send_json_response(response)
            
            elif path == '/prompts':
                # Legacy prompt templates endpoint - now uses database
                from prompt_manager import PromptManager
                pm = PromptManager(db=self.db)
                templates = pm.get_templates()
                self.send_json_response({'success': True, 'templates': templates})
            
            elif path == '/api/experiments':
                experiment_id = self.db.create_experiment(
                    name=data['name'],
                    description=data.get('description', ''),
                    config=data.get('config', {})
                )
                
                experiment = self.db.get_experiment(experiment_id)
                self.send_json_response({'success': True, 'experiment': experiment})
            
            elif path == '/api/prompts':
                prompt_id = self.db.create_prompt(
                    experiment_id=data['experiment_id'],
                    content=data['content'],
                    prompt_type=self.validate_prompt_type(data.get('type', 'base')),
                    parent_prompt_id=data.get('parent_prompt_id'),
                    modifier_used=data.get('modifier_used'),
                    tags=data.get('tags', [])
                )
                
                self.send_json_response({'success': True, 'prompt_id': prompt_id})
            
            elif path == '/api/models':
                model_id = self.db.register_model(
                    name=data['name'],
                    model_type=data.get('type', 'base'),
                    consortium_config=data.get('consortium_config')
                )
                
                self.send_json_response({'success': True, 'model_id': model_id})
            
            elif path == '/api/rankings':
                ranking_id = self.db.save_ranking(
                    experiment_id=data['experiment_id'],
                    prompt_id=data['prompt_id'],
                    generation_id=data['generation_id'],
                    rank=data.get('rank'),
                    quality_score=data.get('quality_score'),
                    evaluator_id=data.get('evaluator_id', 'default')
                )
                
                self.send_json_response({'success': True, 'ranking_id': ranking_id})
            
            else:
                self.send_error_response(f'Unknown endpoint: {path}', 404)
        
        except Exception as e:
            logger.exception(f"Error in POST {path}: {e}")
            self.send_error_response(str(e))
    
    def do_GET(self):
        """Handle GET requests"""
        parsed_path = urlparse(self.path)
        path = parsed_path.path
        logger.info(f"GET request received for: {path}")
        query_params = parse_qs(parsed_path.query)
        
        try:
            if path == '/api/experiments':
                experiments = self.db.list_experiments()
                self.send_json_response({'success': True, 'experiments': experiments})
            
            elif path.startswith('/api/experiments/'):
                parts = path.split('/')
                exp_id = parts[3]
                
                if len(parts) >= 4 and parts[3] == 'export':
                    # Export all experiment data
                    exp_id = query_params.get('experiment_id', [None])[0]
                    if not exp_id:
                        self.send_error_response('No experiment_id provided', 400)
                        return
                    
                    export_data = self.db.export_experiment_data(exp_id)
                    self.send_json_response({'success': True, 'data': export_data})
                else:
                    # Get single experiment
                    experiment = self.db.get_experiment(exp_id)
                    if experiment:
                        self.send_json_response({'success': True, 'experiment': experiment})
                    else:
                        self.send_error_response(f'Experiment not found: {exp_id}', 404)
            
            elif path == '/api/models':
                models = self.db.list_models()
                self.send_json_response({'success': True, 'models': models})
            
            elif path == '/prompts':
                # Legacy endpoint - return templates from PromptManager
                from prompt_manager import PromptManager
                pm = PromptManager(db=self.db)
                templates = pm.get_templates()
                self.send_json_response({'success': True, 'templates': templates})
            
            else:
                self.send_error_response(f'Unknown endpoint: {path}', 404)
        
        except Exception as e:
            logger.exception(f"Error in GET {path}: {e}")
            self.send_error_response(str(e))
    
    def do_PUT(self):
        """Handle PUT requests for updates"""
        path = urlparse(self.path).path
        logger.info(f"PUT request received for: {path}")
        
        try:
            data = self.parse_request_body()
            
            if path == '/prompts':
                # Update prompt templates
                from prompt_manager import PromptManager
                pm = PromptManager(db=self.db)
                
                action = data.get('action')
                if action == 'add':
                    pm.add_template(data['name'], data['template'])
                elif action == 'update':
                    pm.update_template(data['name'], data['template'])
                elif action == 'save_all':
                    pm.save_templates(data['templates'])
                
                self.send_json_response({'success': True})
            
            elif path.startswith('/api/experiments/'):
                exp_id = path.split('/')[3]
                self.db.update_experiment_status(exp_id, data.get('status', 'active'))
                self.send_json_response({'success': True})
            
            else:
                self.send_error_response(f'Unknown endpoint: {path}', 404)
        
        except Exception as e:
            logger.exception(f"Error in PUT {path}: {e}")
            self.send_error_response(str(e))
    
    def do_DELETE(self):
        """Handle DELETE requests"""
        path = urlparse(self.path).path
        logger.info(f"DELETE request received for: {path}")
        
        try:
            if path == '/prompts':
                # Delete prompt template
                data = self.parse_request_body()
                from prompt_manager import PromptManager
                pm = PromptManager(db=self.db)
                pm.delete_template(data['name'])
                self.send_json_response({'success': True})
            
            else:
                self.send_error_response(f'Unknown endpoint: {path}', 404)
        
        except Exception as e:
            logger.exception(f"Error in DELETE {path}: {e}")
            self.send_error_response(str(e))

def main():
    port = 8081
    server = HTTPServer(('localhost', port), EnhancedVibeLab)
    logger.info(f"Starting VibeLab Enhanced Backend on http://localhost:{port}")
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        logger.info("Shutting down server...")
        # Clean up any active LLM processes
        EnhancedVibeLab.llm_executor.cancel_all()
        server.shutdown()

if __name__ == '__main__':
    main()
